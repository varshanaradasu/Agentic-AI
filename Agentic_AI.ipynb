{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6+mu/dZ4mMuH8KatrW+hI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varshanaradasu/Agentic-AI/blob/main/Agentic_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGl6Du_-BhD4",
        "outputId": "e0067e00-8f0f-4d64-8fcc-7a0b8bcdc061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.10)\n",
            "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.12/dist-packages (0.2.6)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.0.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.24)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: chromadb>=1.0.20 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (1.0.21)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (0.17.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (4.25.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb>=1.0.20->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain-chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain-chroma) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (3.3.1)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.20->langchain-chroma) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.20->langchain-chroma) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain-chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain-chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.20->langchain-chroma) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain-chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain-chroma) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain-chroma) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain-chroma) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.20->langchain-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.20->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-google-genai langchain-chroma pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os"
      ],
      "metadata": {
        "id": "7MQ-S5dzvLz2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=PyPDFLoader(\"file.pdf\")\n",
        "docs=loader.load()\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJFLoPOFv-2N",
        "outputId": "47ffc85e-c83a-434c-b1aa-613b1590c710"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-07-07T11:23:18+05:30', 'author': 'JHANSI LAXMI', 'moddate': '2025-07-07T11:23:18+05:30', 'source': 'file.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='Department of Computer Science and Engineering \\nYear & Section: III &       Academic Year: 2025-26 \\nMACHINE LEARNING LAB MANUAL \\nLAB MANUAL – MODULE 1: Supervised Learning & Regression \\n \\nEXPERIMENT 1: Data Preprocessing and Feature Engineering \\nProblem Statement: Given the Titanic Dataset, clean and preprocess the data to make it suitable for \\nclassification. \\nTasks: \\n• Load the dataset using Pandas. \\n• Identify and handle missing values using appropriate strategies. \\n• Encode categorical variables using Label Encoding and One-Hot Encoding. \\n• Split the dataset into train and test sets (80:20). \\n• Apply feature scaling (StandardScaler or MinMaxScaler). \\nExpected Outcome: \\n• Cleaned dataset ready for classification \\n• Report shapes of original and processed datasets \\n• Visualizations of missing data before and after cleaning \\n \\nEXPERIMENT 2: Regression Modeling for House Price Prediction \\nDataset: House Prices - Advanced Regression Techniques \\nProblem Statement: Predict the house sale price based on features like square footage, number of \\nrooms, location, etc. \\nModels to Implement: \\n• Linear Regression (Univariate and Multivariate) \\n• Polynomial Regression \\n• Ridge and LASSO Regression \\nExpected Outcome: \\n• Print model coefficients and intercepts \\n• Plot actual vs predicted values for all models \\n• Report R², MAE, and RMSE for each model \\n• Comment on overfitting/underfitting observations'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-07-07T11:23:18+05:30', 'author': 'JHANSI LAXMI', 'moddate': '2025-07-07T11:23:18+05:30', 'source': 'file.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='EXPERIMENT 3: Heart Disease Classification Using Logistic Regression \\nDataset: Heart Disease UCI Dataset \\nProblem Statement: Predict whether a patient is likely to have heart disease. \\nModels to Implement: \\n• Logistic Regression \\nTasks: \\n• Train the model using 4 validation strategies: \\no Simple hold-out validation \\no K-fold cross validation \\no Stratified K-fold cross validation \\no Leave-One-Out (LOO) validation \\n• Evaluate performance with Accuracy, Precision, Recall, F1 Score \\n• Plot the confusion matrix \\nExpected Outcome: \\n• Tabulate and compare validation scores \\n• Graph performance metrics and confusion matrix \\n \\nEXPERIMENT 4: Feature Selection on a Breast Cancer Dataset \\nDataset: Breast Cancer Wisconsin Dataset \\nProblem Statement: Select the most informative features to predict cancer diagnosis. \\nTasks: \\n• Apply Filter Method: Chi-Square test \\n• Apply Wrapper Method: Forward and Backward Selection \\n• Apply Embedded Method: Elastic Net Regularization \\n• Evaluate model performance with and without feature selection using Logistic Regression \\nExpected Outcome: \\n• List selected features in each method \\n• Tabulate performance comparison with selected vs full feature sets \\n \\nEXPERIMENT 5: Dimensionality Reduction and Impact Analysis'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-07-07T11:23:18+05:30', 'author': 'JHANSI LAXMI', 'moddate': '2025-07-07T11:23:18+05:30', 'source': 'file.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='Dataset: Wine Quality Dataset \\nProblem Statement: Evaluate the effect of dimensionality reduction on classification accuracy. \\nTasks: \\n• Apply PCA and LDA for reducing dimensionality \\n• Train logistic regression and decision tree on: \\no Original dataset \\no PCA-reduced dataset \\no LDA-reduced dataset \\nExpected Outcome: \\n• Compare and tabulate accuracy, F1 Score across three datasets \\n• Visualize decision boundaries (2D PCA/LDA plots) \\n \\nLAB MANUAL – MODULE 2: Classification, Clustering & Ensembles \\n \\nEXPERIMENT 6: Classifiers Comparison \\nDataset: Iris Dataset \\nProblem Statement: Classify iris flower species using various supervised classifiers. \\nModels to Implement: \\n• Decision Tree (CART) \\n• k-Nearest Neighbors (k-NN) \\n• Fuzzy k-NN \\n• Multi-layer Perceptron \\nExpected Outcome: \\n• Report accuracy, precision, recall, F1 score for all models \\n• Visualize decision boundaries (use PCA for 2D projection) \\n \\nEXPERIMENT 7: Ensemble Models for Binary Classification \\nDataset: Bank Marketing Dataset \\nProblem Statement: Predict if a client will subscribe to a term deposit based on campaign data. \\nModels to Implement: \\n• Random Forest (Bagging)'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-07-07T11:23:18+05:30', 'author': 'JHANSI LAXMI', 'moddate': '2025-07-07T11:23:18+05:30', 'source': 'file.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='• AdaBoost and Gradient Boosting \\n• XGBoost \\n• Stacking Classifier \\nExpected Outcome: \\n• Compare performance using ROC-AUC \\n• Print feature importances \\n• Show how ensemble models outperform base classifiers \\n \\nEXPERIMENT 8: Clustering and Evaluation \\nDataset: Mall Customers Dataset \\nProblem Statement: Cluster customers based on spending score and income. \\nTasks: \\n• K-Means Clustering (find k using Elbow method) \\n• Fuzzy C-Means Clustering \\n• Spectral Clustering \\n• Self-Organizing Maps (SOM) \\nExpected Outcome: \\n• Visualize clusters \\n• Evaluate clustering using Silhouette Score and Davies–Bouldin Index \\n \\nEXPERIMENT 9: Handling Imbalanced Data \\nDataset: Credit Card Fraud Detection Dataset \\nProblem Statement: Improve classification performance on a highly imbalanced fraud dataset. \\nTasks: \\n• Visualize class imbalance \\n• Apply Random Over/Under Sampling \\n• Apply SMOTE and ADASYN \\n• Train Decision Tree and Logistic Regression on original and balanced data \\nExpected Outcome: \\n• Compare F1 score, precision, recall on imbalanced vs balanced data \\n• Visualize confusion matrices'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-07-07T11:23:18+05:30', 'author': 'JHANSI LAXMI', 'moddate': '2025-07-07T11:23:18+05:30', 'source': 'file.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='EXPERIMENT 10: Content-Based Recommendation System \\nDataset: MovieLens 100K \\nProblem Statement: Build a movie recommender based on user ratings. \\nTasks: \\n• Use cosine similarity for content-based filtering \\n• Generate top-5 recommendations for a given user \\n• Visualize rating distributions and similarity heatmaps \\nExpected Outcome: \\n• Recommend movies based on user profile \\n• Show similarity matrix and explain recommendation logic \\n \\nSUBMISSION GUIDELINES: \\n• Use Jupyter notebooks with markdown cells for explanation \\n• Include all graphs, metric tables, and observations \\n• Maintain separate notebook per experiment \\n• Submit GitHub or ZIP folder link for evaluation')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter=RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "chunks=splitter.split_documents(docs)\n",
        "print(len(chunks))\n",
        "print(chunks[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPeD6_zCxBOm",
        "outputId": "ac56c107-b681-4937-9f6d-26158b2429e8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "Department of Computer Science and Engineering \n",
            "Year & Section: III &       Academic Year: 2025-26 \n",
            "MACHINE LEARNING LAB MANUAL \n",
            "LAB MANUAL – MODULE 1: Supervised Learning & Regression \n",
            " \n",
            "EXPERIMENT 1: Data Preprocessing and Feature Engineering \n",
            "Problem Statement: Given the Titanic Dataset, clean and preprocess the data to make it suitable for \n",
            "classification. \n",
            "Tasks: \n",
            "• Load the dataset using Pandas. \n",
            "• Identify and handle missing values using appropriate strategies. \n",
            "• Encode categorical variables using Label Encoding and One-Hot Encoding. \n",
            "• Split the dataset into train and test sets (80:20). \n",
            "• Apply feature scaling (StandardScaler or MinMaxScaler). \n",
            "Expected Outcome: \n",
            "• Cleaned dataset ready for classification \n",
            "• Report shapes of original and processed datasets \n",
            "• Visualizations of missing data before and after cleaning \n",
            " \n",
            "EXPERIMENT 2: Regression Modeling for House Price Prediction \n",
            "Dataset: House Prices - Advanced Regression Techniques\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "embeddings=GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\",\n",
        "google_api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "Re56Th2iyzSG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(len(chunks))\n",
        "print(chunks[0].page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7vulsz4LUi2",
        "outputId": "8eb719db-e0f4-4f64-cf91-8df2b1d0829c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "Department of Computer Science and Engineering \n",
            "Year & Section: III &       Academic Year: 2025-26 \n",
            "MACHINE LEARNING LAB MANUAL \n",
            "LAB MANUAL – MODULE 1: Supervised Learning & Regression \n",
            " \n",
            "EXPERIMENT 1: Data Preprocessing and Feature Engineering \n",
            "Problem Statement: Given the Titanic Dataset, clean and preprocess the data to make it suitable for \n",
            "classification. \n",
            "Tasks: \n",
            "• Load the dataset using Pandas. \n",
            "• Identify and handle missing values using appropriate strategies. \n",
            "• Encode categorical variables using Label Encoding and One-Hot Encoding. \n",
            "• Split the dataset into train and test sets (80:20). \n",
            "• Apply feature scaling (StandardScaler or MinMaxScaler). \n",
            "Expected Outcome: \n",
            "• Cleaned dataset ready for classification \n",
            "• Report shapes of original and processed datasets \n",
            "• Visualizations of missing data before and after cleaning \n",
            " \n",
            "EXPERIMENT 2: Regression Modeling for House Price Prediction \n",
            "Dataset: House Prices - Advanced Regression Techniques\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store=Chroma.from_documents(documents=chunks,embedding=embeddings)"
      ],
      "metadata": {
        "id": "egOWh_NPzsWH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":5})"
      ],
      "metadata": {
        "id": "eQHrQ-Pqz14k"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model=ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.9,\n",
        "    max_output_tokens=500\n",
        "    )"
      ],
      "metadata": {
        "id": "VWj8IPxD0COd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=PromptTemplate(\n",
        "    template=\"\"\"\n",
        "    You are a helpful assistant.\n",
        "    Answer from the provided transcript context..\n",
        "\n",
        "    {context}\n",
        "    Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables=[\"context\",\"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "hq281mDi1WNE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'what are tasks for experiment 3'\n",
        "retrieved_docs = retriever.invoke(question)\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
      ],
      "metadata": {
        "id": "-5N86nLt1WFd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = prompt.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})\n",
        "final_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92EjskvkLret",
        "outputId": "32d5175a-f3ac-475b-ff7b-9ce15c5bd51e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='\\n    You are a helpful assistant.\\n    Answer from the provided transcript context..\\n\\n    EXPERIMENT 3: Heart Disease Classification Using Logistic Regression \\nDataset: Heart Disease UCI Dataset \\nProblem Statement: Predict whether a patient is likely to have heart disease. \\nModels to Implement: \\n• Logistic Regression \\nTasks: \\n• Train the model using 4 validation strategies: \\no Simple hold-out validation \\no K-fold cross validation \\no Stratified K-fold cross validation \\no Leave-One-Out (LOO) validation \\n• Evaluate performance with Accuracy, Precision, Recall, F1 Score \\n• Plot the confusion matrix \\nExpected Outcome: \\n• Tabulate and compare validation scores \\n• Graph performance metrics and confusion matrix \\n \\nEXPERIMENT 4: Feature Selection on a Breast Cancer Dataset \\nDataset: Breast Cancer Wisconsin Dataset \\nProblem Statement: Select the most informative features to predict cancer diagnosis. \\nTasks: \\n• Apply Filter Method: Chi-Square test \\n• Apply Wrapper Method: Forward and Backward Selection \\n• Apply Embedded Method: Elastic Net Regularization\\n\\nEXPERIMENT 3: Heart Disease Classification Using Logistic Regression \\nDataset: Heart Disease UCI Dataset \\nProblem Statement: Predict whether a patient is likely to have heart disease. \\nModels to Implement: \\n• Logistic Regression \\nTasks: \\n• Train the model using 4 validation strategies: \\no Simple hold-out validation \\no K-fold cross validation \\no Stratified K-fold cross validation \\no Leave-One-Out (LOO) validation \\n• Evaluate performance with Accuracy, Precision, Recall, F1 Score \\n• Plot the confusion matrix \\nExpected Outcome: \\n• Tabulate and compare validation scores \\n• Graph performance metrics and confusion matrix \\n \\nEXPERIMENT 4: Feature Selection on a Breast Cancer Dataset \\nDataset: Breast Cancer Wisconsin Dataset \\nProblem Statement: Select the most informative features to predict cancer diagnosis. \\nTasks: \\n• Apply Filter Method: Chi-Square test \\n• Apply Wrapper Method: Forward and Backward Selection \\n• Apply Embedded Method: Elastic Net Regularization\\n\\n• Visualizations of missing data before and after cleaning \\n \\nEXPERIMENT 2: Regression Modeling for House Price Prediction \\nDataset: House Prices - Advanced Regression Techniques \\nProblem Statement: Predict the house sale price based on features like square footage, number of \\nrooms, location, etc. \\nModels to Implement: \\n• Linear Regression (Univariate and Multivariate) \\n• Polynomial Regression \\n• Ridge and LASSO Regression \\nExpected Outcome: \\n• Print model coefficients and intercepts \\n• Plot actual vs predicted values for all models \\n• Report R², MAE, and RMSE for each model \\n• Comment on overfitting/underfitting observations\\n\\n• Visualizations of missing data before and after cleaning \\n \\nEXPERIMENT 2: Regression Modeling for House Price Prediction \\nDataset: House Prices - Advanced Regression Techniques \\nProblem Statement: Predict the house sale price based on features like square footage, number of \\nrooms, location, etc. \\nModels to Implement: \\n• Linear Regression (Univariate and Multivariate) \\n• Polynomial Regression \\n• Ridge and LASSO Regression \\nExpected Outcome: \\n• Print model coefficients and intercepts \\n• Plot actual vs predicted values for all models \\n• Report R², MAE, and RMSE for each model \\n• Comment on overfitting/underfitting observations\\n\\nDepartment of Computer Science and Engineering \\nYear & Section: III &       Academic Year: 2025-26 \\nMACHINE LEARNING LAB MANUAL \\nLAB MANUAL – MODULE 1: Supervised Learning & Regression \\n \\nEXPERIMENT 1: Data Preprocessing and Feature Engineering \\nProblem Statement: Given the Titanic Dataset, clean and preprocess the data to make it suitable for \\nclassification. \\nTasks: \\n• Load the dataset using Pandas. \\n• Identify and handle missing values using appropriate strategies. \\n• Encode categorical variables using Label Encoding and One-Hot Encoding. \\n• Split the dataset into train and test sets (80:20). \\n• Apply feature scaling (StandardScaler or MinMaxScaler). \\nExpected Outcome: \\n• Cleaned dataset ready for classification \\n• Report shapes of original and processed datasets \\n• Visualizations of missing data before and after cleaning \\n \\nEXPERIMENT 2: Regression Modeling for House Price Prediction \\nDataset: House Prices - Advanced Regression Techniques\\n    Question: what are tasks for experiment 3\\n    ')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()\n",
        "\n",
        "# Generate the answer\n",
        "response = chat_model.invoke(final_prompt)\n",
        "\n",
        "parser.invoke(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xx9zcbzzLvN1",
        "outputId": "55d13641-52df-40ba-999a-c68cb9342c86"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The tasks for Experiment 3 are:\\n\\n*   Train the model using 4 validation strategies:\\n    *   Simple hold-out validation\\n    *   K-fold cross validation\\n    *   Stratified K-fold cross validation\\n    *   Leave-One-Out (LOO) validation\\n*   Evaluate performance with Accuracy, Precision, Recall, F1 Score\\n*   Plot the confusion matrix'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}